---
title: "mini-project-2"
output: html_document
author: "Akash Mahajan (akashmjn@stanford.edu), Raunaq Rewari (raunaq@stanford.edu)"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
sapply(c('ggplot2','data.table','tidyverse','DT','gridExtra','forecast','xts'),require,character.only=TRUE)
```

## Recap - Spotify's Worldwide Daily Song Ranking

To recall, we are working with a dataset containing streams for the daily top 200 songs, over a span of 226 days in 2017 (Starting Jan 1st), containing 53 regions totally, as well as global charts. Our data in its raw form looks as below, consisting of Date, Region, TrackName Streams, Artist, Position (on top 200), URL. This corresponds to 916600 rows totally, with the URL serving as a unqiue ID for a track.  

To simplify our problem, we focus our attention on only the top 20 regions, effectively those having mean daily streams of $\geq\sim 13,000$. 

```{r dataprep, echo=FALSE}

# loading in script files
source("src_akash.R")

dt = fread("data.csv")  # faster than read.csv 
setnames(dt,"Track Name","TrackName")
dt[,Date:=as.Date(Date)]
data = data.frame(dt)
data$Date = as.Date(data$Date)

# Filter to just top 20 regions
dt = getTopNRegions(dt,20)

head(dt[Region=="global"][Date=="2017-08-17"][order(-Streams)],5)

# datatable(head(dt[Region=="global"][Date=="2017-08-17"][order(-Streams)],20),options = list(pageLength=5))

```

## Problem Formulation

### Transformation / Discretization 

While our dataset looks deceptively simple in form, it is also fairly unstructured, consisting of about ~19000 partial time series values for each unique (Track,Region) in the recorded period. These are partial, since a song may enter/leave the top 200 for only a small period of time. Thus building on some of the open questions from our previous report, we've done some further exploration before posing our problem. Our primary motivation is - can we spot any potential trends in song streams that indicate a rise in popularity, to say Top 50? 

We first make sure that we are working with songs that only have fairly complete durations of streams present. Looking at how the tracks are distributed, we confirm our expectation that a good number are present for a very small number of days (<10-15). We arbitrarily set a threshold at d=28 days (i.e. 4 weeks), bringing down the total number of tracks/time series to ~8300. 

```{r tsdurations, echo=FALSE}

dtTSDurations = dt[,.(DaysInCharts=.N),by=.(URL,Region)]

p1 <- ggplot(dtTSDurations)+geom_histogram(aes(x=DaysInCharts),bins=50)+ylab("Histogram of tracks")
p2 <- ggplot(dtTSDurations)+stat_ecdf(aes(x=DaysInCharts))+ylab("CDF of Tracks")

grid.arrange(p1,p2,nrow=1)

# Filtering down data to only time series of a minium duration
# Giving a unique ID to a (Track,Region) pair. (this corresponds to one time series)
tsFilterStat = filterValidTS(dt,28)
dtFilteredTS = tsFilterStat[[1]]
dtTSDurations = tsFilterStat[[2]]

```

Finally, we split these songs into discrete categories, based on an arbitrary popularity category such as Top 50, 100, etc. (Note that top 200 constitutes our entire dataset). These tracks are then randomly sampled, to constitute our training/test datasets.  

For illustration purposes, trends for songs that have made it to the top 50 at some point over our period of interest, are plotted below. 

```{r regionvariation, echo=FALSE}

# filter tracks by chart positions (top 200 tracks, top 100 tracks, etc.)
topUniqueTracks = data.table()
for(N in c(200,100,50,20,5)){
  stat = getTopNTracks(dtFilteredTS,N)
  topNTracksByRegion = stat[[2]]
  topUniqueTracks = rbind(topUniqueTracks,
                          topNTracksByRegion[,
                                      .(UniqueTracks=.N,Category=paste0("Top",N)),
                                      by=Region])
}

# plotting unique songs by region
ggplot(topUniqueTracks)+
  geom_bar(aes(x=reorder(Region,-UniqueTracks),
               y=UniqueTracks,fill=Category),stat='identity',position = 'dodge')

## Looking at behavior 
top50Stat = getTopNTracks(dtFilteredTS,50)
dtTop50 = top50Stat[[1]]

# filter and only keep tracks that have made a rise in the charts
# i.e. min Position of track is less that threshold at some point 

regionVec = c('gb','se','us')

ggplot(dtTop50[Region%in%regionVec])+
  geom_line(aes(x=Date,y=Streams,group=TrackName,alpha=0.2,color=Position))+
  facet_wrap(~Region,scale="free_y",nrow=length(regionVec))

```

### Formulating prediction tasks

* Classification: Given external data about track from the Spotify API, such as genre, artist, and the region from our dataset, can we predict the category labels? 
* Regression: For songs in the top 5, can we forecast the number of streams, looking ahead a certain period (say 10 days)? 

The classification task output would be useful to explore at a high-level what correlates with song popularity. 

The regression task output could be useful in helping estimate how long a song that has made it to the top of the charts actually stays there. 

## Prediction Progress 

### Classification task

#### Baseline


### Regression task 

#### Baseline 

A baseline for this task would be to just predict the mean of the data. We use the RMSE as an evaluation metric to compare against. Our RMSE is 143479.02 (this is high since the scale of our data is relatively high as well). 

```{r regress1, echo=FALSE}

## Making a dataset of only top 5 songs 
nTop = 5
topStat = getTopNTracks(dtFilteredTS,5)
dtTop = topStat[[1]]

dtTopGB = dtTop[Region=='gb']
gbTopTSIDs = unique(dtTopGB$TSID)

set.seed(3)
trainTSIDs = sample(gbTopTSIDs,31)

dtTS = dtTopGB[TSID==trainTSIDs[1],.(Streams,Date)]
trainTS1 = xts(dtTS[,Streams],order.by = dtTS[,Date])

# Printing RMSE
RMSEBaseline = sqrt(dtTopGB[TSID%in%trainTSIDs,(Streams-mean(Streams))^2,by=TSID][,mean(V1)])

print(paste("RMSE for baseline:",round(RMSEBaseline,2) ))

```


#### Approach

For an initial simple analysis, we choose just the GB region with about 41 tracks that have made it to the top 5. We randomly set aside (25\%) ~ 10 tracks that we will not use during our model building purposes. 

We plan to explore the use of time series models for forecasting these streams. We make use of an ARIMA model that effectively regresses on values at a previous time instance. An ARMA(p,q,d) model can be expressed as below (while the I component refers to differencing of the series, done to eliminate a non-zero mean)

\[
X_t - \alpha_1X_{t-1}, \cdots, -\alpha_pX_{t-p} = \epsilon_t +\theta_1\epsilon_{t-1},\cdots,+\theta_q\epsilon_{t-q} 
\]

To get an idea of model performance, we just evaluate a best fitted model on a randomly chosen sample of the training songs. 

Model selection (between differently parameterized models for p,q,d), is done using in-sample estimates of the generalization error, namely the AIC via the R package used. 

```{r regress2, echo=FALSE}

tsModel = auto.arima(trainTS1)

summary(tsModel)

ggplot(dtTS)+
  geom_line(aes(x=Date,y=Streams))+
  geom_line(aes(x=Date,y=tsModel$fitted),color='red')

plot(forecast(tsModel))

```

While as expected, the RMSE for our model is lower than the baseline, we need further on dealing with a couple of challenged as outlined below. 

### Open Questions

Going forward from this point to the next stage in our analysis, i.e. modelling the song popularities there are a few questions that need to be addressed. 

1. How do we valide time series models for many different tracks? Do we fit many individual time series models? Do we fit one model per region? 

2. 

