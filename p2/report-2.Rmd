---
title: "mini-project-2"
author: "Akash Mahajan (akashmjn@stanford.edu), Raunaq Rewari (raunaq@stanford.edu)"
output:
  pdf_document: 
    keep_tex: yes
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
sapply(c('ggplot2','data.table','tidyverse','DT','gridExtra','forecast','xts'),require,character.only=TRUE)
```

## Recap - Spotify's Worldwide Daily Song Ranking

To recall, we are working with a dataset containing streams for the daily top 200 songs, over a span of 226 days in 2017 (Starting Jan 1st), containing 53 regions totally, as well as global charts. Our data in its raw form looks as below, consisting of Date, Region, TrackName Streams, Artist, Position (on top 200), URL. This corresponds to 916600 rows totally, with the URL serving as a unqiue ID for a track.  

To simplify our problem, we focus our attention on only the top 20 regions, effectively those having mean daily streams of $\geq\sim 13,000$. 

```{r dataprep, echo=FALSE}

# loading in script files
source("src_akash.R")

dt = fread("data.csv")  # faster than read.csv 
setnames(dt,"Track Name","TrackName")
dt[,Date:=as.Date(Date)]
data = data.frame(dt)
data$Date = as.Date(data$Date)

# Filter to just top 20 regions
dt = getTopNRegions(dt,20)

head(dt[Region=="global"][Date=="2017-08-17"][order(-Streams)],5)

# datatable(head(dt[Region=="global"][Date=="2017-08-17"][order(-Streams)],20),options = list(pageLength=5))

```

## Problem Formulation

### Transformation / Discretization 

While our dataset looks deceptively simple in form, it is also fairly unstructured, consisting of about ~19000 partial time series values for each unique (Track,Region) in the recorded period. These are partial, since a song may enter/leave the top 200 for only a small period of time. Thus building on some of the open questions from our previous report, we've done some further exploration before posing our problem. Our primary motivation is - can we spot any potential trends in song streams that indicate a rise in popularity, to say Top 50? 

We first make sure that we are working with songs that only have fairly complete durations of streams present. Looking at how the tracks are distributed, we confirm our expectation that a good number are present for a very small number of days (<10-15). We arbitrarily set a threshold at d=28 days (i.e. 4 weeks), bringing down the total number of tracks/time series to ~8300. 

```{r tsdurations, echo=FALSE,fig.height=3}

dtTSDurations = dt[,.(DaysInCharts=.N),by=.(URL,Region)]

p1 <- ggplot(dtTSDurations)+geom_histogram(aes(x=DaysInCharts),bins=50)+ylab("Histogram of tracks")
p2 <- ggplot(dtTSDurations)+stat_ecdf(aes(x=DaysInCharts))+ylab("CDF of Tracks")

grid.arrange(p1,p2,nrow=1)

# Filtering down data to only time series of a minium duration
# Giving a unique ID to a (Track,Region) pair. (this corresponds to one time series)
tsFilterStat = filterValidTS(dt,28)
dtFilteredTS = tsFilterStat[[1]]
dtTSDurations = tsFilterStat[[2]]

```

Finally, we split these songs into discrete categories, based on an arbitrary popularity category such as Top 50, 100, etc. (Note that top 200 constitutes our entire dataset). These tracks are then randomly sampled, to constitute our training/test datasets.  

For illustration purposes, trends for songs that have made it to the top 50 at some point over our period of interest, are plotted below. 

```{r regionvariation, echo=FALSE}

# filter tracks by chart positions (top 200 tracks, top 100 tracks, etc.)
topUniqueTracks = data.table()
for(N in c(200,100,50,20,5)){
  stat = getTopNTracks(dtFilteredTS,N)
  topNTracksByRegion = stat[[2]]
  topUniqueTracks = rbind(topUniqueTracks,
                          topNTracksByRegion[,
                                      .(UniqueTracks=.N,Category=paste0("Top",N)),
                                      by=Region])
}

# plotting unique songs by region
p1 <- ggplot(topUniqueTracks)+
  geom_bar(aes(x=reorder(Region,-UniqueTracks),
               y=UniqueTracks,fill=Category),stat='identity',position = 'dodge')

## Looking at behavior 
top50Stat = getTopNTracks(dtFilteredTS,50)
dtTop50 = top50Stat[[1]]

# filter and only keep tracks that have made a rise in the charts
# i.e. min Position of track is less that threshold at some point 

regionVec = c('gb','se','us')

p2 <- ggplot(dtTop50[Region%in%regionVec])+
  geom_line(aes(x=Date,y=Streams,group=TrackName,alpha=0.2,color=Position))+
  facet_wrap(~Region,scale="free_y",nrow=length(regionVec))

grid.arrange(p1,p2,ncol=1)

```

### Formulating prediction tasks

* Classification: Given external data about track from the Spotify API, such as genre, artist, and the region from our dataset, can we predict the category labels? 
* Regression: For songs in the top 5, can we forecast the number of streams, looking ahead a certain period (say 10 days)? 

The classification task output would be useful to explore at a high-level what correlates with song popularity. 

The regression task output could be useful in helping estimate how long a song that has made it to the top of the charts actually stays there. 

## Prediction Progress 

### Regression task 

#### Baseline 

A baseline for this task would be to just predict the mean of the data. We use the RMSE as an evaluation metric to compare against. Our RMSE is 143479.02 (this is high since the scale of our data is relatively high as well). 

```{r regress1, echo=FALSE}

## Making a dataset of only top 5 songs 
nTop = 5
topStat = getTopNTracks(dtFilteredTS,5)
dtTop = topStat[[1]]

dtTopGB = dtTop[Region=='gb']
gbTopTSIDs = unique(dtTopGB$TSID)

set.seed(3)
trainTSIDs = sample(gbTopTSIDs,31)

dtTS = dtTopGB[TSID==trainTSIDs[1],.(Streams,Date)]
trainTS1 = xts(dtTS[,Streams],order.by = dtTS[,Date])

# Printing RMSE
RMSEBaseline = sqrt(dtTopGB[TSID%in%trainTSIDs,(Streams-mean(Streams))^2,by=TSID][,mean(V1)])

print(paste("RMSE for baseline:",round(RMSEBaseline,2) ))

```


#### Approach

For an initial simple analysis, we choose just the GB region with about 41 tracks that have made it to the top 5. We randomly set aside (25\%) ~ 10 tracks that we will not use during our model building purposes. 

We plan to explore the use of time series models for forecasting these streams. We make use of an ARIMA model that effectively regresses on values at a previous time instance. An ARMA(p,q,d) model can be expressed as below (while the I component refers to differencing of the series, done to eliminate a non-zero mean)

\[
X_t - \alpha_1X_{t-1}, \cdots, -\alpha_pX_{t-p} = \epsilon_t +\theta_1\epsilon_{t-1},\cdots,+\theta_q\epsilon_{t-q} 
\]

To get an idea of model performance, we just evaluate a best fitted model on a randomly chosen sample of the training songs. 

Model selection (between differently parameterized models for p,q,d), is done using in-sample estimates of the generalization error, namely the AIC via the R package used. 

```{r regress2, echo=FALSE,fig.height=3}

tsModel = auto.arima(trainTS1)

summary(tsModel)

ggplot(dtTS)+
  geom_line(aes(x=Date,y=Streams))+
  geom_line(aes(x=Date,y=tsModel$fitted),color='red')

plot(forecast(tsModel))

```

While as expected, the RMSE for our model is lower than the baseline, we need further work on dealing with a couple of challenges as outlined below. 

### Open Questions

While we have only done a very basic exploration, there's fair amount of work we need to do to tie this together. 

1. How do we valide time series models for many different tracks? Do we fit many individual time series models? Do we fit one model per region? 

2. A classification task on our dataset seems a little difficult to pose, hence for the classification setting, we looked at trying to predict top 20 potential for a song based on the audio features of the song.

To get the audio features, we used Spotify’s API that returns the following features, given the song id:

	•	danceability
	•	energy
	•	key
	•	loudness
	•	mode
	•	speechiness
	•	acousticness
	•	instrumentalness
	•	liveness
	•	valence
	•	tempo

For this task, we also modified the dataset to make it amenable to the classification setting. Any song that had a position within top 20 was given a label 1 and 0 otherwise.

We have not included the classification results here (even though we have attached the relevant code) because we weren’t confident about the results. This was mainly because the same song (with the same features) is represented multiple times in the dataset, across multiple regions.

We are working on making modifications for the same to conclude our analysis. 

\newpage
## Source Code

```{r sourcecode, echo=TRUE,eval=FALSE}

##### Helper functions used #######

# Filter and return top N regions 
getTopNRegions <- function(dt,N){
  # Mean of streams by region
  dailyStreamsByRegion = dt[,.(MeanStreams=mean(Streams)),by=Region]
  # filter by region - only top 20 regions
  topNRegions = dailyStreamsByRegion[order(-MeanStreams)][1:(N+1)]
  dtFiltered = merge(dt,topNRegions,by = "Region")
  return(dtFiltered)
}

# Filter only tracks that have been in the top N 
# charts by region. N=200 will just return the entire dataset
# Returns: list( dtFiltered, topNTracksStats )
getTopNTracks <- function(dt,N){
  # filter tracks getting <= N ranking at some point
  # groupby TrackName, Region gives a count of dates 
  topNTracksByRegion = dt[Position<=N][,
                               .(DaysInTopN=.N),
                               by=.(TSID,Region)]
  # join and filter only tracks in this topN list
  dtFiltered = merge(dt,topNTracksByRegion,by=c("TSID","Region"))
  return(list(dtFiltered,topNTracksByRegion))
}

# filter valid time series 
# Returns: list( dtFiltered, dtTSDurations )
filterValidTS <- function(dt,minDaysThresh){
  # pulling out time series with minimum days present 
  dtTSDurations = dt[,.(DaysInCharts=.N),by=.(URL,Region)][DaysInCharts>=minDaysThresh]
  # giving each time series and ID
  dtTSDurations[,TSID:=.I]
  # joining and filtering back on original dataset
  dtFiltered = merge(dt,dtTSDurations[,.(TSID,Region,URL)],by = c("URL","Region"))
  return(list(dtFiltered,dtTSDurations))
}

######  Notebook code ########

## Data preparation 

# loading in script files
source("src_akash.R")

dt = fread("data.csv")  # faster than read.csv 
setnames(dt,"Track Name","TrackName")
dt[,Date:=as.Date(Date)]
data = data.frame(dt)
data$Date = as.Date(data$Date)

# Filter to just top 20 regions
dt = getTopNRegions(dt,20)

## Filtering time series by minimum durations

dtTSDurations = dt[,.(DaysInCharts=.N),by=.(URL,Region)]

p1 <- ggplot(dtTSDurations)+geom_histogram(aes(x=DaysInCharts),bins=50)+ylab("Histogram of tracks")
p2 <- ggplot(dtTSDurations)+stat_ecdf(aes(x=DaysInCharts))+ylab("CDF of Tracks")

grid.arrange(p1,p2,nrow=1)

# Filtering down data to only time series of a minium duration
# Giving a unique ID to a (Track,Region) pair. (this corresponds to one time series)
tsFilterStat = filterValidTS(dt,28)
dtFilteredTS = tsFilterStat[[1]]
dtTSDurations = tsFilterStat[[2]]

## Track Categories

# filter tracks by chart positions (top 200 tracks, top 100 tracks, etc.)
topUniqueTracks = data.table()
for(N in c(200,100,50,20,5)){
  stat = getTopNTracks(dtFilteredTS,N)
  topNTracksByRegion = stat[[2]]
  topUniqueTracks = rbind(topUniqueTracks,
                          topNTracksByRegion[,
                                      .(UniqueTracks=.N,Category=paste0("Top",N)),
                                      by=Region])
}

### Regression 

## Making a dataset of only top 5 songs 
nTop = 5
topStat = getTopNTracks(dtFilteredTS,5)
dtTop = topStat[[1]]

dtTopGB = dtTop[Region=='gb']
gbTopTSIDs = unique(dtTopGB$TSID)

set.seed(3)
trainTSIDs = sample(gbTopTSIDs,31)

dtTS = dtTopGB[TSID==trainTSIDs[1],.(Streams,Date)]
trainTS1 = xts(dtTS[,Streams],order.by = dtTS[,Date])

# Printing RMSE
RMSEBaseline = sqrt(dtTopGB[TSID%in%trainTSIDs,(Streams-mean(Streams))^2,by=TSID][,mean(V1)])

print(paste("RMSE for baseline:",round(RMSEBaseline,2) ))

tsModel = auto.arima(trainTS1)

summary(tsModel)

ggplot(dtTS)+
  geom_line(aes(x=Date,y=Streams))+
  geom_line(aes(x=Date,y=tsModel$fitted),color='red')

plot(forecast(tsModel))

### Classification 

# Number of unique songs in the top x 

x = 20
unique_top_20 = dt %>% group_by(Region) %>% filter(Position %in% c(1:x)) %>% 
  summarise(Total_Unique = n_distinct(Track.Name))

# Doing a trends analysis for the song "Chantaje" 
time_series = dt %>% group_by(Track.Name, Region) %>% filter(Track.Name == "Chantaje")
top_regions = (time_series %>% group_by(Region) %>% 
                 summarise(total = n()) %>% filter(total == max(total)))$Region

time_series %>% filter(Region %in% top_regions) %>% 
  ggplot() + geom_point(mapping = aes(x = Date, y = Position, color = Region))

# Make new dataset with song related features
song_feats = read_csv("song_feats.csv")
merged = merge(dt, song_feats, by="URL")
merged$label = cut(merged$Position, breaks=c(0,20,200), labels=c(1,0))

# Run logistic regression
classification = glm(label ~ energy + liveness + tempo + speechiness + acousticness + instrumentalness + danceability + loudness, data = merged, family = binomial)


```

